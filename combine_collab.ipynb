{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMY3MkDQmLE4AEnXkBXHJeX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zheien/FYP-2024-2025S2/blob/main/combine_collab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bac3e1ad"
      },
      "source": [
        "!pip install jiwer editdistance accelerate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxwpKvyF-FkS"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from jiwer import cer,wer\n",
        "import editdistance\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import transformers\n",
        "\n",
        "# print(\"[DEBUG] transformers version:\", transformers.__version__)\n",
        "# print(\"[DEBUG] TrainingArguments location:\", transformers.TrainingArguments.__module__)\n",
        "# print(\"[DEBUG] TrainingArguments class:\", transformers.TrainingArguments)\n",
        "# print(\"[DEBUG] dir(transformers.TrainingArguments):\", dir(transformers.TrainingArguments))\n",
        "\n",
        "output_dir = os.path.expanduser(\"~/scratch/combine-qwen\")\n",
        "os.makedirs(output_dir, exist_ok=True)  # Ensure the directory exists\n",
        "def format_chat(example, tokenizer, inference=False):\n",
        "    try:\n",
        "        input_text = f\"### Input\\n{example['normalized']}\"\n",
        "    except KeyError:\n",
        "        print(f\"Missing 'normalized' key in example: {example}\")\n",
        "        raise\n",
        "\n",
        "    if not inference:\n",
        "        #\n",
        "        target_text = f\"### Output\\n{example['unnormalized']}<|endoftext|>\"\n",
        "        full_text = input_text + \"\\n\" + target_text\n",
        "\n",
        "        # Tokenize full sequence (prompt + response)\n",
        "        full_ids = tokenizer.encode(full_text, truncation=True, max_length=tokenizer.model_max_length)\n",
        "\n",
        "        # Tokenize only the prompt to figure out how many tokens to ignore\n",
        "        input_ids_prompt_only = tokenizer.encode(input_text, truncation=True, max_length=tokenizer.model_max_length)\n",
        "\n",
        "        # Create labels: ignore prompt tokens using -100\n",
        "        labels = [-100] * len(input_ids_prompt_only) + full_ids[len(input_ids_prompt_only):]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": full_ids,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "    else:\n",
        "        # In inference mode, just return the input prompt\n",
        "        input_ids = tokenizer.encode(\n",
        "            input_text,\n",
        "            truncation=True,\n",
        "            max_length=tokenizer.model_max_length\n",
        "        )\n",
        "        return {\n",
        "            \"input_ids\": input_ids\n",
        "        }\n",
        "\n",
        "def prepare_training_data(data_list, tokenizer, device):\n",
        "\n",
        "    dataset = Dataset.from_list(data_list)\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda example: format_chat(example, tokenizer),\n",
        "        batched=False\n",
        "    )\n",
        "\n",
        "    print(\"Data preparation complete.\")\n",
        "    print(tokenized_dataset)\n",
        "    return tokenized_dataset\n",
        "\n",
        "def prepare_validation_data(data_list, tokenizer, device):\n",
        "    print(\"Starting validation data preparation...\")\n",
        "    dataset = Dataset.from_list(data_list)\n",
        "\n",
        "    tokenized_dataset = dataset.map(\n",
        "        lambda example: format_chat(example, tokenizer),\n",
        "        batched=False\n",
        "    )\n",
        "\n",
        "    print(\"Validation data preparation complete.\")\n",
        "    print(tokenized_dataset)\n",
        "    return tokenized_dataset\n",
        "\n",
        "def main():\n",
        "    import os\n",
        "\n",
        "    file_name = os.environ.get(\"FILE_NAME\")\n",
        "    print(\"Loading the model and tokenizer...\")\n",
        "    model_name = \"Qwen/Qwen3-0.6B-Base\"\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    print(f\"Model is running on: {device}\")\n",
        "\n",
        "    # Load training data\n",
        "    training_data_path = \"datasets/combined_train.json\"\n",
        "    with open(training_data_path, \"r\") as f:\n",
        "        train_data = json.load(f)\n",
        "\n",
        "    # training_data = Dataset.from_list(raw_data)\n",
        "    # train_dataset = load_dataset(\"json\", data_files=training_data_path, split=\"train\", streaming=True)\n",
        "\n",
        "    # Load validation data\n",
        "    validation_data_path = \"datasets/combined_val.json\"\n",
        "    with open(validation_data_path, \"r\") as f:\n",
        "        validation_data  = json.load(f)\n",
        "\n",
        "    # Load test data\n",
        "    # test_data_path = os.path.join(os.path.dirname(__file__), \"datasets\", \"combined_test.json\")\n",
        "    # with open(test_data_path, \"r\") as f:\n",
        "    #     test_data  = json.load(f)\n",
        "\n",
        "    # Prepare datasets\n",
        "#     train_shard = training_data.shard(num_shards=num_shards, index=shard_index)\n",
        "#     print(f\"ðŸ“Š Number of samples in train_shard: {len(train_shard)}\")\n",
        "#     train_set = train_shard.map(\n",
        "#     lambda example: format_chat(example, tokenizer),\n",
        "#     remove_columns=[],\n",
        "#     batched=False,\n",
        "# )\n",
        "#     print(train_set)\n",
        "\n",
        "    # train_set = train_shard.map(prepare_training_data)\n",
        "\n",
        "    train_dataset = prepare_training_data(train_data, tokenizer, device)\n",
        "\n",
        "\n",
        "\n",
        "    validation_dataset = prepare_validation_data(validation_data , tokenizer, device)\n",
        "\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=3,\n",
        "        # max_steps=1000,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=8,\n",
        "        learning_rate=1e-4,\n",
        "        warmup_steps=10000,\n",
        "        # logging_steps=1000,\n",
        "        save_steps=50000,\n",
        "        save_strategy=\"steps\",\n",
        "        bf16=True,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=50000,\n",
        "        dataloader_num_workers=4,\n",
        "\n",
        "    )\n",
        "\n",
        "    # data_collator = DataCollatorForLanguageModeling(\n",
        "    # tokenizer=tokenizer,\n",
        "    # mlm=False,\n",
        "    # pad_to_multiple_of=8,\n",
        "    # return_tensors=\"pt\",\n",
        "    # # padding=True\n",
        "    # )\n",
        "    data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=validation_dataset,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "    except Exception as e:\n",
        "        print(f\"Training interrupted: {e}\")\n",
        "\n",
        "    print(\"Saving the finetuned model...\")\n",
        "    model.save_pretrained(\"./combine-qwen\")\n",
        "    tokenizer.save_pretrained(\"./combine-qwen\")\n",
        "\n",
        "    print(\"Evaluating the model on multiple test datasets...\")\n",
        "    test_files = [\n",
        "        (\"ami\", \"datasets/ami_test.json\"),\n",
        "        (\"swbd\", \"datasets/swbd_test.json\"),\n",
        "        (\"earnings\", \"datasets/earnings_test.json\"),\n",
        "        (\"chime\", \"datasets/chime_test.json\"),\n",
        "        (\"gtn\", \"datasets/gtn_test.json\"),\n",
        "        (\"spgi\", \"datasets/spgi_test.json\")\n",
        "    ]\n",
        "\n",
        "    for test_name, test_file_path in test_files:\n",
        "        print(f\"\\nðŸš€ Evaluating on {test_name}...\")\n",
        "\n",
        "        try:\n",
        "            with open(test_file_path, \"r\") as f:\n",
        "                test_data = json.load(f)\n",
        "        except FileNotFoundError:\n",
        "            print(f\"[WARN] Test file {test_file_path} not found. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        total_edits = 0\n",
        "        total_chars = 0\n",
        "\n",
        "        output_path = f\"eval_{test_name}.txt\"\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"--- Evaluation results for {test_name} ---\\n\\n\")\n",
        "\n",
        "            for i, raw_example in enumerate(test_data):\n",
        "                try:\n",
        "                    eval_input = format_chat(raw_example, tokenizer, inference=True)\n",
        "                    input_ids = torch.tensor(eval_input['input_ids']).unsqueeze(0).to(device)\n",
        "                    attention_mask = (input_ids != tokenizer.eos_token_id).long()\n",
        "\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        output_ids = model.generate(\n",
        "                            input_ids=input_ids,\n",
        "                            attention_mask=attention_mask,\n",
        "                            max_new_tokens=256\n",
        "                        )\n",
        "                        decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "                        generated_text = decoded.split(\"### Response\")[-1].strip() if \"### Response\" in decoded else decoded.strip()\n",
        "                        # strip <|endoftext|> from generated output before evaluation\n",
        "                        generated_text = generated_text.replace(\"<|endoftext|>\", \"\").strip()\n",
        "\n",
        "                    expected_text = raw_example['unnormalized']\n",
        "                    edits = editdistance.eval(generated_text, expected_text)\n",
        "                    cer = edits / len(expected_text) if len(expected_text) > 0 else 0\n",
        "                    total_edits += edits\n",
        "                    total_chars += len(expected_text)\n",
        "\n",
        "                    # Print to terminal\n",
        "                    # print(f\"\\n[Test Set: {test_name} | Sample {i}]\")\n",
        "                    # print(f\"Expected: {expected_text}\")\n",
        "                    # print(f\"Generated: {generated_text}\")\n",
        "                    # print(f\"CER: {cer:.4f}\")\n",
        "\n",
        "                    # Write to file\n",
        "                    # f.write(f\"[Sample {i}]\\n\")\n",
        "                    f.write(f\"Expected : {expected_text}\\n\")\n",
        "                    f.write(f\"Generated: {generated_text}\\n\")\n",
        "                    f.write(f\"CER      : {cer:.4f}\\n\\n\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"[ERROR] {test_name} test sample {i} failed: {e}\")\n",
        "                    f.write(f\"[Sample {i}] ERROR: {e}\\n\\n\")\n",
        "\n",
        "            global_cer = total_edits / total_chars if total_chars > 0 else 0\n",
        "            print(f\"âœ… {test_name} CER: {global_cer:.4f}\")\n",
        "            f.write(f\"\\nâœ… Global CER on {test_name}: {global_cer:.4f}\\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Script started...\")\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd8e9b19"
      },
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Create the datasets directory\n",
        "os.makedirs(\"datasets\", exist_ok=True)\n",
        "\n",
        "# Create dummy JSON files for training and validation\n",
        "train_data = [\n",
        "    {\"normalized\": \"hello world\", \"unnormalized\": \"Hello, World!\"},\n",
        "    {\"normalized\": \"this is a test\", \"unnormalized\": \"This is a test.\"}\n",
        "]\n",
        "\n",
        "validation_data = [\n",
        "    {\"normalized\": \"another test\", \"unnormalized\": \"Another test.\"},\n",
        "    {\"normalized\": \"one more\", \"unnormalized\": \"One more.\"}\n",
        "]\n",
        "\n",
        "with open(\"datasets/combined_train.json\", \"w\") as f:\n",
        "    json.dump(train_data, f)\n",
        "\n",
        "with open(\"datasets/combined_val.json\", \"w\") as f:\n",
        "    json.dump(validation_data, f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}